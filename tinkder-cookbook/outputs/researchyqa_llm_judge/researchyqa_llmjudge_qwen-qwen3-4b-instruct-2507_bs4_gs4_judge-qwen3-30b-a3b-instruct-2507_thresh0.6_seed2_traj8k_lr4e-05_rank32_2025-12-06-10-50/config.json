{
  "learning_rate": 4e-05,
  "dataset_builder": {
    "batch_size": 4,
    "group_size": 4,
    "model_name_for_tokenizer": "Qwen/Qwen3-4B-Instruct-2507",
    "renderer_name": "qwen3_instruct",
    "chroma_tool_config": {
      "chroma_host": "localhost",
      "chroma_port": 8000,
      "chroma_collection_name": "researchyqa_corpus",
      "retrieval_config": {
        "n_results": 3,
        "embedding_config": {
          "model_name": "gemini-embedding-001",
          "embedding_dim": 768,
          "task_type": "RETRIEVAL_QUERY"
        }
      },
      "max_retries": 10,
      "initial_retry_delay": 1
    },
    "judge_model_name": "Qwen/Qwen3-30B-A3B-Instruct-2507",
    "judge_model_path": null,
    "judge_rubric": "Consider the following aspects when evaluating the answer:\n\n1. **Factual Accuracy (Highest Priority)**: Is the information correct and verifiable against the reference answers?\n2. **Relevance**: Does the answer directly address the question asked?\n3. **Completeness**: Are all important aspects of the question covered adequately?\n4. **Use of Retrieved Context**: Does the answer appropriately incorporate information from the retrieved documents?\n5. **Clarity and Structure**: Is the answer well-organized, concise, and easy to understand?\n\nPrioritize factual accuracy and relevance. An answer that is factually correct but less eloquent \nshould score higher than a well-written but inaccurate answer.",
    "convo_prefix": "standard",
    "seed": 2,
    "max_eval_size": 1024,
    "max_trajectory_tokens": 8192,
    "quality_threshold": 0.6,
    "questions_path": "CS329x_Final/ResearchyQA/ResearchyQA_questions_with_answers_100.jsonl"
  },
  "model_name": "Qwen/Qwen3-4B-Instruct-2507",
  "max_tokens": 1024,
  "compute_post_kl": false,
  "evaluator_builders": [],
  "lora_rank": 32,
  "kl_penalty_coef": 0.0,
  "kl_discount_factor": 0.0,
  "loss_fn": "importance_sampling",
  "num_substeps": 1,
  "wandb_project": "researchyqa-llm-judge",
  "wandb_name": "researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed2_traj8k_lr4e-05_rank32_2025-12-06-10-50",
  "log_path": "/Users/yanzhenshen/Desktop/CS329x/tinker-cookbook/outputs/researchyqa_llm_judge/researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed2_traj8k_lr4e-05_rank32_2025-12-06-10-50",
  "base_url": null,
  "enable_trace": false,
  "remove_constant_reward_groups": false,
  "eval_every": 0,
  "save_every": 20,
  "load_checkpoint_path": null,
  "async_config": null,
  "stream_minibatch_config": null,
  "num_groups_to_log": 4
}