_wandb:
    value:
        cli_version: 0.22.3
        e:
            hv71eabpgigpnpvqr6d7r52a3ppmkbr3:
                apple:
                    ecpuCores: 6
                    gpuCores: 10
                    memoryGb: 16
                    name: Apple M4
                    pcpuCores: 4
                    ramTotalBytes: "17179869184"
                    swapTotalBytes: "2147483648"
                args:
                    - model_name=Qwen/Qwen3-4B-Instruct-2507
                    - lora_rank=32
                    - judge_model_name=Qwen/Qwen3-30B-A3B-Instruct-2507
                    - judge_model_path=tinker://2b33e94d-bdea-4c21-bdb7-2af143b79c8e/sampler_weights/final
                    - |-
                      judge_rubric=Evaluate the answer primarily based on the following two critical aspects:

                      1. **Evidence-Based Reasoning with Source Citations**: CRITICAL - Does the answer explicitly reference or cite the retrieved sources for EVERY claim? The model MUST NOT make unsupported claims. High-quality answers should indicate which information comes from which searches, using phrases like "According to the search results...", "The retrieved documents indicate...", or "Based on the information found...". Any claim not supported by the retrieved context should be penalized. The reasoning should be transparently grounded in the evidence retrieved from the search tool.

                      2. **Multiple Perspectives & Balanced Coverage**: CRITICAL - For questions that may have different viewpoints, are controversial, or involve debates, does the answer explicitly present multiple perspectives and acknowledge different viewpoints? High-quality answers should include phrases like "Some argue..., while others contend...", "From one perspective..., from another...", or "Proponents believe..., whereas critics argue...". This is especially important for topics involving politics, ethics, social issues, or scientific debates.

                      If the answer fails on either of these dimensions (e.g., makes unsupported claims, lacks citations, or ignores alternative viewpoints on a controversial topic), it should receive a low score regardless of its other qualities.
                    - |
                      system_prompt=
                      You are an expert assistant who solves tasks using a Wikipedia search tool and provides balanced, nuanced answers.
                      Tool calling. Execute the tool by wrapping calls in <function_call>...</function_call>

                      The search tool you are given has the following schema:
                      ```
                      {
                          "name": "search",
                          "title": "Wikipedia search",
                          "description": "Searches Wikipedia for relevant information based on the given query.",
                          "inputSchema": {
                              "type": "object",
                              "properties": {
                                  "query_list": {
                                      "type": "array",
                                      "items": {"type": "string"},
                                      "description": "A list of fully-formed semantic queries. The tool will return search results for each query.",
                                  }
                              },
                              "required": ["query_list"],
                          },
                          "outputSchema": {
                              "type": "string",
                              "description": "The search results in JSON format",
                          },
                      }
                      ```

                      Here are instructions for how to solve a problem:
                      1. Think step by step before calling the tool and after you receive the result of the tool call. Decide what queries to call the tool with.
                      2. IMPORTANT: For questions that may be controversial or have multiple perspectives (e.g., about politics, ethics, social issues, scientific debates), actively search for different viewpoints by using queries that capture contrasting perspectives (e.g., "arguments for X", "criticisms of X", "debate on X").
                      3. Call the tool with the queries you have decided on.
                      4. Think step by step again after you receive the result of the tool call. If you have the information you need, you can stop here.
                      5. Otherwise, come up with new queries that combine information from the previous results.
                      6. IMPORTANT: When formulating your final answer, you MUST explicitly cite the retrieved sources and evidence for every claim to avoid making unsupported statements. Use phrases like "According to the search results...", "The retrieved documents indicate...", or "Based on the information found...". DO NOT make any claims that are not supported by the retrieved documents. Make it clear that your reasoning is grounded in the evidence you retrieved.
                      7. When formulating your final answer, if the topic is controversial or has different viewpoints, explicitly present multiple perspectives using language like "Some argue..., while others contend..." or "From one perspective..., from another..." or "Proponents believe..., whereas critics argue...". This provides a more complete and balanced response.
                      8. Include your final answer after the "Answer:" prefix. The answer should be comprehensive yet concise, with clear references to the retrieved evidence.

                      Here is an example of solving a real question:
                      "Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?"

                      1. Think step by step: In order to answer this question, I need to know the population of New York City and San Francisco between 2020 and 2025. I will search for the population of New York City in each year
                      2. Calling search tool: <function_call>{"name": "search", "args": {"query_list": ["Population New York city between 2020 and 2025"]}}</function_call> (Output omitted for brevity)
                      3. Think step by step again: I have the population of New York City in each year, and I see that the population of New York City grew the most in 2024. I need to know the population of San Francisco in 2024. I will search for the population of San Francisco in each year.
                      <function_call>{"name": "search", "args": {"query_list": ["Population San Francisco between 2023 and 2024"]}}</function_call> (Output omitted for brevity)
                      4. Answer: The population of New York City grew the most in 2024, and the population of San Francisco changed by XXXX in 2024.
                    - learning_rate=4e-5
                    - batch_size=4
                    - group_size=4
                    - seed=42
                    - max_tokens=1024
                    - eval_every=0
                    - max_trajectory_tokens=8192
                    - quality_threshold=0.6
                    - questions_path=CS329x_Final/DebateQA_training/DebateQA_questions_200.jsonl
                    - wandb_name=debateqa_multiperspective_cite_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-5_rank32_2025-12-07-12-42
                cpu_count: 10
                cpu_count_logical: 10
                disk:
                    /:
                        total: "494384795648"
                        used: "122928803840"
                email: yanzhen4@stanford.edu
                executable: /opt/miniconda3/envs/cs329x_hw2/bin/python
                git:
                    commit: 20e26a629797188aa8c6f34474b0d4757b20b90d
                    remote: https://github.com/thinking-machines-lab/tinker-cookbook.git
                host: Yanzhens-MacBook-Air.local
                memory:
                    total: "17179869184"
                os: macOS-15.6-arm64-arm-64bit
                program: -m tinker_cookbook.recipes.tool_use.search.train_researchyqa_llm_judge
                python: CPython 3.11.14
                root: /Users/yanzhenshen/Desktop/CS329x/tinker-cookbook/outputs/researchyqa_llm_judge/researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-05_rank32_2025-12-07-12-42
                startedAt: "2025-12-07T20:42:25.813028Z"
                writerId: hv71eabpgigpnpvqr6d7r52a3ppmkbr3
        m: []
        python_version: 3.11.14
        t:
            "1":
                - 1
                - 49
                - 92
                - 105
            "2":
                - 1
                - 11
                - 49
                - 92
                - 105
            "3":
                - 13
                - 16
                - 61
            "4": 3.11.14
            "5": 0.22.3
            "12": 0.22.3
            "13": darwin-arm64
async_config:
    value: null
base_url:
    value: null
compute_post_kl:
    value: false
dataset_builder:
    value:
        batch_size: 4
        chroma_tool_config:
            chroma_collection_name: researchyqa_corpus
            chroma_host: localhost
            chroma_port: 8000
            initial_retry_delay: 1
            max_retries: 10
            retrieval_config:
                embedding_config:
                    embedding_dim: 768
                    model_name: gemini-embedding-001
                    task_type: RETRIEVAL_QUERY
                n_results: 3
        convo_prefix: standard
        group_size: 4
        judge_model_name: Qwen/Qwen3-30B-A3B-Instruct-2507
        judge_model_path: tinker://2b33e94d-bdea-4c21-bdb7-2af143b79c8e/sampler_weights/final
        judge_rubric: |-
            Evaluate the answer primarily based on the following two critical aspects:

            1. **Evidence-Based Reasoning with Source Citations**: CRITICAL - Does the answer explicitly reference or cite the retrieved sources for EVERY claim? The model MUST NOT make unsupported claims. High-quality answers should indicate which information comes from which searches, using phrases like "According to the search results...", "The retrieved documents indicate...", or "Based on the information found...". Any claim not supported by the retrieved context should be penalized. The reasoning should be transparently grounded in the evidence retrieved from the search tool.

            2. **Multiple Perspectives & Balanced Coverage**: CRITICAL - For questions that may have different viewpoints, are controversial, or involve debates, does the answer explicitly present multiple perspectives and acknowledge different viewpoints? High-quality answers should include phrases like "Some argue..., while others contend...", "From one perspective..., from another...", or "Proponents believe..., whereas critics argue...". This is especially important for topics involving politics, ethics, social issues, or scientific debates.

            If the answer fails on either of these dimensions (e.g., makes unsupported claims, lacks citations, or ignores alternative viewpoints on a controversial topic), it should receive a low score regardless of its other qualities.
        max_eval_size: 1024
        max_trajectory_tokens: 8192
        model_name_for_tokenizer: Qwen/Qwen3-4B-Instruct-2507
        quality_threshold: 0.6
        questions_path: CS329x_Final/DebateQA_training/DebateQA_questions_200.jsonl
        renderer_name: qwen3_instruct
        seed: 42
        system_prompt: |4
            You are an expert assistant who solves tasks using a Wikipedia search tool and provides balanced, nuanced answers.
            Tool calling. Execute the tool by wrapping calls in <function_call>...</function_call>

            The search tool you are given has the following schema:
            ```
            {
                "name": "search",
                "title": "Wikipedia search",
                "description": "Searches Wikipedia for relevant information based on the given query.",
                "inputSchema": {
                    "type": "object",
                    "properties": {
                        "query_list": {
                            "type": "array",
                            "items": {"type": "string"},
                            "description": "A list of fully-formed semantic queries. The tool will return search results for each query.",
                        }
                    },
                    "required": ["query_list"],
                },
                "outputSchema": {
                    "type": "string",
                    "description": "The search results in JSON format",
                },
            }
            ```

            Here are instructions for how to solve a problem:
            1. Think step by step before calling the tool and after you receive the result of the tool call. Decide what queries to call the tool with.
            2. IMPORTANT: For questions that may be controversial or have multiple perspectives (e.g., about politics, ethics, social issues, scientific debates), actively search for different viewpoints by using queries that capture contrasting perspectives (e.g., "arguments for X", "criticisms of X", "debate on X").
            3. Call the tool with the queries you have decided on.
            4. Think step by step again after you receive the result of the tool call. If you have the information you need, you can stop here.
            5. Otherwise, come up with new queries that combine information from the previous results.
            6. IMPORTANT: When formulating your final answer, you MUST explicitly cite the retrieved sources and evidence for every claim to avoid making unsupported statements. Use phrases like "According to the search results...", "The retrieved documents indicate...", or "Based on the information found...". DO NOT make any claims that are not supported by the retrieved documents. Make it clear that your reasoning is grounded in the evidence you retrieved.
            7. When formulating your final answer, if the topic is controversial or has different viewpoints, explicitly present multiple perspectives using language like "Some argue..., while others contend..." or "From one perspective..., from another..." or "Proponents believe..., whereas critics argue...". This provides a more complete and balanced response.
            8. Include your final answer after the "Answer:" prefix. The answer should be comprehensive yet concise, with clear references to the retrieved evidence.

            Here is an example of solving a real question:
            "Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?"

            1. Think step by step: In order to answer this question, I need to know the population of New York City and San Francisco between 2020 and 2025. I will search for the population of New York City in each year
            2. Calling search tool: <function_call>{"name": "search", "args": {"query_list": ["Population New York city between 2020 and 2025"]}}</function_call> (Output omitted for brevity)
            3. Think step by step again: I have the population of New York City in each year, and I see that the population of New York City grew the most in 2024. I need to know the population of San Francisco in 2024. I will search for the population of San Francisco in each year.
            <function_call>{"name": "search", "args": {"query_list": ["Population San Francisco between 2023 and 2024"]}}</function_call> (Output omitted for brevity)
            4. Answer: The population of New York City grew the most in 2024, and the population of San Francisco changed by XXXX in 2024.
enable_trace:
    value: false
eval_every:
    value: 0
evaluator_builders:
    value: []
kl_discount_factor:
    value: 0
kl_penalty_coef:
    value: 0
learning_rate:
    value: 4e-05
load_checkpoint_path:
    value: null
log_path:
    value: /Users/yanzhenshen/Desktop/CS329x/tinker-cookbook/outputs/researchyqa_llm_judge/researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-05_rank32_2025-12-07-12-42
lora_rank:
    value: 32
loss_fn:
    value: importance_sampling
max_tokens:
    value: 1024
model_name:
    value: Qwen/Qwen3-4B-Instruct-2507
num_groups_to_log:
    value: 4
num_substeps:
    value: 1
remove_constant_reward_groups:
    value: false
save_every:
    value: 20
stream_minibatch_config:
    value: null
wandb_name:
    value: debateqa_multiperspective_cite_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-5_rank32_2025-12-07-12-42
wandb_project:
    value: researchyqa-llm-judge
