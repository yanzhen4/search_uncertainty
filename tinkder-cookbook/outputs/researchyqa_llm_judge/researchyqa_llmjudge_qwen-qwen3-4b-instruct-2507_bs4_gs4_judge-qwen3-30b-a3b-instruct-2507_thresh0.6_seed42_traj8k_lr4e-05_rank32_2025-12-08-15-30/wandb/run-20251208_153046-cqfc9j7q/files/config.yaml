_wandb:
    value:
        cli_version: 0.22.3
        e:
            miabf4w062nw24ezoc0l321hae0gq53c:
                apple:
                    ecpuCores: 6
                    gpuCores: 10
                    memoryGb: 16
                    name: Apple M4
                    pcpuCores: 4
                    ramTotalBytes: "17179869184"
                    swapTotalBytes: "4294967296"
                args:
                    - model_name=Qwen/Qwen3-4B-Instruct-2507
                    - lora_rank=32
                    - judge_model_name=Qwen/Qwen3-30B-A3B-Instruct-2507
                    - judge_model_path=tinker://2b33e94d-bdea-4c21-bdb7-2af143b79c8e/sampler_weights/final
                    - |-
                      judge_rubric=Evaluate the answer based on the following aspects:

                      1. **Search Tool Usage & Strategy**: How effectively did the model use the search tool? High-quality responses demonstrate strategic search behavior:
                         - Multiple searches (typically 2-4) covering different aspects of the question
                         - Well-formulated queries that target specific information needs
                         - Follow-up searches that build upon initial results to fill gaps
                         - Appropriate search breadth for complex or multi-faceted questions

                      2. **Document-Based Reasoning & Citation**: How well does the answer use and cite retrieved documents?
                         - Explicit document citations using numbered references (e.g., "According to Document 1...", "Document 2 indicates...")
                         - All claims are grounded in specific retrieved documents, not speculation
                         - Proper attribution showing which document supports each claim
                         - Synthesizes information from multiple documents with clear citations
                         - Does NOT make unsupported claims or add information not found in the retrieved documents

                      3. **Balanced Perspective & Completeness**: For topics with multiple viewpoints:
                         - Acknowledges different perspectives from different documents (e.g., "Document 1 argues... while Document 3 contends...")
                         - Presents controversial topics fairly by citing multiple document sources
                         - Indicates when documents show debates or disagreements
                         - Addresses multiple aspects of complex questions using evidence from retrieved documents

                      Scoring guidance:
                      - 0.8-1.0: Excellent search strategy, strong document-based reasoning with explicit citations (e.g., "Document 1", "Document 2"), balanced perspective
                      - 0.6-0.7: Good search usage, mostly document-based with citations, generally balanced
                      - 0.4-0.5: Adequate searches but weaker citation practice, some unsupported claims or vague references
                      - 0.2-0.3: Limited search effectiveness, weak document grounding, missing or poor citations
                      - 0.0-0.1: Poor quality, failed to cite documents properly, or made claims without document support
                    - "system_prompt=\nYou are an expert research assistant who uses a Wikipedia search tool to find accurate, well-sourced information. You MUST base your answers on the retrieved documents and cite them properly.\n\nTool calling: Execute the tool by wrapping calls in <tool_call>...</tool_call>\n\nThe search tool you have access to:\n```\n{\n    \"name\": \"search\",\n    \"title\": \"Wikipedia search\",\n    \"description\": \"Searches Wikipedia for relevant information based on the given query.\",\n    \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query_list\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"A list of fully-formed semantic queries. The tool will return search results for each query.\",\n            }\n        },\n        \"required\": [\"query_list\"],\n    },\n    \"outputSchema\": {\n        \"type\": \"string\",\n        \"description\": \"The search results in JSON format\",\n    },\n}\n```\n\nHow to approach research questions:\n\n1. **Plan your searches**: Think about what information you need. Formulate 1-3 initial search queries that target key aspects of the question.\n\n2. **Execute searches**: Call the search tool with your queries:\n   <tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"query1\", \"query2\", ...]}}</tool_call>\n   \n   The tool will return numbered documents (Document 1, Document 2, Document 3, etc.) for each query.\n\n3. **Assess and iterate**: Review the results. Do you have enough information? If not, perform follow-up searches to fill gaps or explore different angles.\n\n4. **Consider multiple perspectives**: For complex or controversial topics (politics, ethics, debates), search for different viewpoints using queries like \"arguments for X\", \"criticisms of X\", \"perspectives on X\".\n\n5. **Answer ONLY based on retrieved documents**: Your answer MUST be grounded in the documents returned by the search tool. Do NOT make claims without document support.\n\n6. **Cite documents by number**: When referencing information, explicitly cite which document(s) you are using. Use formats like:\n   - \"According to Document 1, ...\"\n   - \"Document 2 indicates that ...\"\n   - \"As stated in Document 3 and Document 5, ...\"\n   - \"Based on the information in Document 1, ...\"\n\n7. **Present balanced views**: When documents present multiple perspectives, acknowledge them: \"Document 1 argues... while Document 3 contends...\" or \"Documents 2 and 4 suggest... however, Document 5 points out...\".\n\n8. **Format your answer**: Present your final answer after the \"Answer:\" prefix with proper document citations.\n\nExample workflow:\nQuestion: \"Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?\"\n\nThink: I need NYC and SF population data for 2020-2025.\n<tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"New York City population growth 2020-2025\", \"San Francisco population change 2020-2025\"]}}</tool_call>\n\n[Tool returns: Document 1 with NYC data, Document 2 with SF data]\n\n(After results) Think: Document 1 shows NYC peak was 2024. Need more specific SF 2024 data.\n<tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"San Francisco population 2024\"]}}</tool_call>\n\n[Tool returns: Document 3 with detailed SF 2024 data]\n\nAnswer: According to Document 1, New York City saw the most population growth in 2024 with an increase of [X]. Document 3 indicates that San Francisco's population changed by [Y] in 2024, showing [increase/decrease].\n"
                    - learning_rate=4e-5
                    - batch_size=4
                    - group_size=4
                    - seed=42
                    - max_tokens=1024
                    - eval_every=0
                    - max_trajectory_tokens=8192
                    - quality_threshold=0.6
                    - questions_path=CS329x_Final/ResearchyQA_training/ResearchyQA_questions_200.jsonl
                    - wandb_name=researchyqa_multiperspective_cite_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-5_rank32_2025-12-08-15-30
                cpu_count: 10
                cpu_count_logical: 10
                disk:
                    /:
                        total: "494384795648"
                        used: "125867290624"
                email: yanzhen4@stanford.edu
                executable: /opt/miniconda3/envs/cs329x_hw2/bin/python
                git:
                    commit: 20e26a629797188aa8c6f34474b0d4757b20b90d
                    remote: https://github.com/thinking-machines-lab/tinker-cookbook.git
                host: Yanzhens-MacBook-Air.local
                memory:
                    total: "17179869184"
                os: macOS-15.6-arm64-arm-64bit
                program: -m tinker_cookbook.recipes.tool_use.search.train_llm_judge
                python: CPython 3.11.14
                root: /Users/yanzhenshen/Desktop/CS329x/tinker-cookbook/outputs/researchyqa_llm_judge/researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-05_rank32_2025-12-08-15-30
                startedAt: "2025-12-08T23:30:46.203194Z"
                writerId: miabf4w062nw24ezoc0l321hae0gq53c
        m: []
        python_version: 3.11.14
        t:
            "1":
                - 1
                - 49
                - 92
                - 105
            "2":
                - 1
                - 11
                - 49
                - 92
                - 105
            "3":
                - 13
                - 16
            "4": 3.11.14
            "5": 0.22.3
            "12": 0.22.3
            "13": darwin-arm64
async_config:
    value: null
base_url:
    value: null
compute_post_kl:
    value: false
dataset_builder:
    value:
        batch_size: 4
        chroma_tool_config:
            chroma_collection_name: researchyqa_corpus
            chroma_host: localhost
            chroma_port: 8000
            initial_retry_delay: 1
            max_retries: 10
            retrieval_config:
                embedding_config:
                    embedding_dim: 768
                    model_name: gemini-embedding-001
                    task_type: RETRIEVAL_QUERY
                n_results: 3
        convo_prefix: standard
        group_size: 4
        judge_model_name: Qwen/Qwen3-30B-A3B-Instruct-2507
        judge_model_path: tinker://2b33e94d-bdea-4c21-bdb7-2af143b79c8e/sampler_weights/final
        judge_rubric: |-
            Evaluate the answer based on the following aspects:

            1. **Search Tool Usage & Strategy**: How effectively did the model use the search tool? High-quality responses demonstrate strategic search behavior:
               - Multiple searches (typically 2-4) covering different aspects of the question
               - Well-formulated queries that target specific information needs
               - Follow-up searches that build upon initial results to fill gaps
               - Appropriate search breadth for complex or multi-faceted questions

            2. **Document-Based Reasoning & Citation**: How well does the answer use and cite retrieved documents?
               - Explicit document citations using numbered references (e.g., "According to Document 1...", "Document 2 indicates...")
               - All claims are grounded in specific retrieved documents, not speculation
               - Proper attribution showing which document supports each claim
               - Synthesizes information from multiple documents with clear citations
               - Does NOT make unsupported claims or add information not found in the retrieved documents

            3. **Balanced Perspective & Completeness**: For topics with multiple viewpoints:
               - Acknowledges different perspectives from different documents (e.g., "Document 1 argues... while Document 3 contends...")
               - Presents controversial topics fairly by citing multiple document sources
               - Indicates when documents show debates or disagreements
               - Addresses multiple aspects of complex questions using evidence from retrieved documents

            Scoring guidance:
            - 0.8-1.0: Excellent search strategy, strong document-based reasoning with explicit citations (e.g., "Document 1", "Document 2"), balanced perspective
            - 0.6-0.7: Good search usage, mostly document-based with citations, generally balanced
            - 0.4-0.5: Adequate searches but weaker citation practice, some unsupported claims or vague references
            - 0.2-0.3: Limited search effectiveness, weak document grounding, missing or poor citations
            - 0.0-0.1: Poor quality, failed to cite documents properly, or made claims without document support
        max_eval_size: 1024
        max_trajectory_tokens: 8192
        model_name_for_tokenizer: Qwen/Qwen3-4B-Instruct-2507
        quality_threshold: 0.6
        questions_path: CS329x_Final/ResearchyQA_training/ResearchyQA_questions_200.jsonl
        renderer_name: qwen3_instruct
        seed: 42
        system_prompt: "\nYou are an expert research assistant who uses a Wikipedia search tool to find accurate, well-sourced information. You MUST base your answers on the retrieved documents and cite them properly.\n\nTool calling: Execute the tool by wrapping calls in <tool_call>...</tool_call>\n\nThe search tool you have access to:\n```\n{\n    \"name\": \"search\",\n    \"title\": \"Wikipedia search\",\n    \"description\": \"Searches Wikipedia for relevant information based on the given query.\",\n    \"inputSchema\": {\n        \"type\": \"object\",\n        \"properties\": {\n            \"query_list\": {\n                \"type\": \"array\",\n                \"items\": {\"type\": \"string\"},\n                \"description\": \"A list of fully-formed semantic queries. The tool will return search results for each query.\",\n            }\n        },\n        \"required\": [\"query_list\"],\n    },\n    \"outputSchema\": {\n        \"type\": \"string\",\n        \"description\": \"The search results in JSON format\",\n    },\n}\n```\n\nHow to approach research questions:\n\n1. **Plan your searches**: Think about what information you need. Formulate 1-3 initial search queries that target key aspects of the question.\n\n2. **Execute searches**: Call the search tool with your queries:\n   <tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"query1\", \"query2\", ...]}}</tool_call>\n   \n   The tool will return numbered documents (Document 1, Document 2, Document 3, etc.) for each query.\n\n3. **Assess and iterate**: Review the results. Do you have enough information? If not, perform follow-up searches to fill gaps or explore different angles.\n\n4. **Consider multiple perspectives**: For complex or controversial topics (politics, ethics, debates), search for different viewpoints using queries like \"arguments for X\", \"criticisms of X\", \"perspectives on X\".\n\n5. **Answer ONLY based on retrieved documents**: Your answer MUST be grounded in the documents returned by the search tool. Do NOT make claims without document support.\n\n6. **Cite documents by number**: When referencing information, explicitly cite which document(s) you are using. Use formats like:\n   - \"According to Document 1, ...\"\n   - \"Document 2 indicates that ...\"\n   - \"As stated in Document 3 and Document 5, ...\"\n   - \"Based on the information in Document 1, ...\"\n\n7. **Present balanced views**: When documents present multiple perspectives, acknowledge them: \"Document 1 argues... while Document 3 contends...\" or \"Documents 2 and 4 suggest... however, Document 5 points out...\".\n\n8. **Format your answer**: Present your final answer after the \"Answer:\" prefix with proper document citations.\n\nExample workflow:\nQuestion: \"Between 2020 and 2025, which year did New York City see the most population growth and how did San Francisco population change in that year?\"\n\nThink: I need NYC and SF population data for 2020-2025.\n<tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"New York City population growth 2020-2025\", \"San Francisco population change 2020-2025\"]}}</tool_call>\n\n[Tool returns: Document 1 with NYC data, Document 2 with SF data]\n\n(After results) Think: Document 1 shows NYC peak was 2024. Need more specific SF 2024 data.\n<tool_call>{\"name\": \"search\", \"args\": {\"query_list\": [\"San Francisco population 2024\"]}}</tool_call>\n\n[Tool returns: Document 3 with detailed SF 2024 data]\n\nAnswer: According to Document 1, New York City saw the most population growth in 2024 with an increase of [X]. Document 3 indicates that San Francisco's population changed by [Y] in 2024, showing [increase/decrease].\n"
enable_trace:
    value: false
eval_every:
    value: 0
evaluator_builders:
    value: []
kl_discount_factor:
    value: 0
kl_penalty_coef:
    value: 0
learning_rate:
    value: 4e-05
load_checkpoint_path:
    value: null
log_path:
    value: /Users/yanzhenshen/Desktop/CS329x/tinker-cookbook/outputs/researchyqa_llm_judge/researchyqa_llmjudge_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-05_rank32_2025-12-08-15-30
lora_rank:
    value: 32
loss_fn:
    value: importance_sampling
max_tokens:
    value: 1024
model_name:
    value: Qwen/Qwen3-4B-Instruct-2507
num_groups_to_log:
    value: 4
num_substeps:
    value: 1
remove_constant_reward_groups:
    value: false
save_every:
    value: 20
stream_minibatch_config:
    value: null
wandb_name:
    value: researchyqa_multiperspective_cite_qwen-qwen3-4b-instruct-2507_bs4_gs4_judge-qwen3-30b-a3b-instruct-2507_thresh0.6_seed42_traj8k_lr4e-5_rank32_2025-12-08-15-30
wandb_project:
    value: researchyqa-llm-judge
